{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNagxAjBCx4Q4oqH3kos8/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/code_practice/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sequence to Sequence Learning with Neural Networks (NIPS 2014) 실습**\n",
        "\n",
        "- 본 코드는 **Attention을 이용하지 않은 Seq2Seq** 모델을 다룹니다.\n",
        "- 코드 실행 전에 **[런타임] → [런타임 유형 변경]** → 유형을 **GPU**로 설정합니다\n",
        "- code by: https://github.com/graykode/nlp-tutorial?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "4BywhEq4hq9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xgfnPSqINept"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "    input_batch, output_batch, target_batch = [], [], []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        for i in range(2):  # 0~1\n",
        "            seq[i] = seq[i] + 'P'*(n_step - len(seq[i]))    # 패딩\n",
        "\n",
        "        input = [num_dic[n] for n in seq[0]]    # char2index\n",
        "        output = [num_dic[n] for n in ('S' + seq[1])]   # teacher forcing을 위한 decoding 입력\n",
        "        target = [num_dic[n] for n in (seq[1] + 'E')]   # 실제로 예측해야하는 정답 sequence\n",
        "\n",
        "        input_batch.append(np.eye(n_class)[input])  # input에 해당하는 word의 one-hot vector\n",
        "        output_batch.append(np.eye(n_class)[output])    # (Teacher forcing) input에 해당하는 word의 one-hot vector\n",
        "        target_batch.append(target) # not one-hot\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "# Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.fc = nn.Linear(n_hidden, n_class)\n",
        "\n",
        "    def forward(self, enc_input, enc_hidden, dec_input):\n",
        "        \"\"\"\n",
        "        enc_input: [batch_size, max_len(=n_step; time step), n_class]\n",
        "                     [배치크기, 단어의최대길이, 원-핫 벡터라 dimension이 n_class]\n",
        "        enc_hidden: [num_layers * num_directions, batch_size, n_hidden]\n",
        "        dec_input: [batch_size, max_len+1(=n_step; time step)(because of 'S' or 'E'), n_class]\n",
        "        \"\"\"\n",
        "        enc_input = enc_input.transpose(0,1)    # enc_input: [max_len(=n_step; time step), batch_size, n_class]\n",
        "        dec_input = dec_input.transpose(0, 1)   # dec_input: [max_len(=n_step; time step), batch_size, n_class]\n",
        "\n",
        "        # enc_states: [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
        "        # outputs: [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
        "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
        "\n",
        "        model = self.fc(outputs)    # model: [max_len+1(=6), batch_size, n_class]\n",
        "        return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5  # 최대 단어 길이\n",
        "    n_hidden = 128\n",
        "\n",
        "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
        "    num_dic = {n:i for i, n in enumerate(char_arr)} # char2index\n",
        "    n_class = len(num_dic)\n",
        "    seq_data = [['man','women'],\n",
        "                ['black','white'],\n",
        "                ['king', 'queen'],\n",
        "                ['girl','boy'],\n",
        "                ['up','down'],\n",
        "                ['high','low']]\n",
        "    batch_size = len(seq_data)\n",
        "\n",
        "    # input_batch: [batch_size, max_len(=n_step; time step), n_class]\n",
        "                #  [배치크기, 단어의최대길이, 원-핫 벡터라 dimension이 n_class]\n",
        "    # output_batch: [batch_size, max_len+1(=n_step; time step)(because of 'S' or 'E'), n_class]\n",
        "    # target_batch: [batch_size, max_len+1(n_step; time step)], not one-hot\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    model = Seq2Seq()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    for epoch in range(5000):\n",
        "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # output = [max_len+1, batch_size, n_class]\n",
        "        output = model(input_batch, hidden, output_batch)\n",
        "        output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class]\n",
        "        loss = 0\n",
        "        for i in range(0, len(target_batch)):\n",
        "            # output[i]: [max_len+1, n_class]\n",
        "            # target_batch[i]: [max_len+1]\n",
        "            loss += criterion(output[i], target_batch[i])   # 문자별로 loss 계산\n",
        "        if (epoch + 1)%1000 == 0:\n",
        "            print(f'Epoch: {epoch+1:#04d} cost:{loss:.6f}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    def make_testbach(input_word):\n",
        "        \"\"\"\n",
        "        parameter:\n",
        "            input_word: str\n",
        "        return:\n",
        "            torch.FloatTensor(input_batch).unsqueeze(0): [1, max_len(=n_step), n_class],\n",
        "            torch.FloatTensor(output_batch).unsqueeze(0): [1, max_len+1, n_class]\n",
        "        \"\"\"\n",
        "        input_batch, output_batch = [], []\n",
        "\n",
        "        input_w = input_word + 'P'*(n_step - len(input_word))\n",
        "        input = [num_dic[n] for n in input_w]   # 1개의 word를 char2index\n",
        "        output = [num_dic[n] for n in 'S' + 'P'*n_step]\n",
        "\n",
        "        input_batch = np.eye(n_class)[input]    # one-hot\n",
        "        output_batch = np.eye(n_class)[output]  # one-hot\n",
        "\n",
        "        return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
        "\n",
        "    def translate(word):\n",
        "        \"\"\"\n",
        "        word: str\n",
        "        \"\"\"\n",
        "        input_batch, output_batch = make_testbach(word)\n",
        "\n",
        "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "        hidden = torch.zeros(1, 1, n_hidden)\n",
        "        # output: [max_len+1(=6), batch_size(=1), n_class]\n",
        "                                  # word가 1개가 들어가기 때문에, batch=1\n",
        "        output = model(input_batch, hidden, output_batch)\n",
        "        predict = torch.argmax(output, dim=2)   # logit은 n_class에 적혀있으므로, dim=2를 기준으로 구함\n",
        "        decoded = [char_arr[i] for i in predict]\n",
        "        end = decoded.index('E')\n",
        "        translated = ''.join(decoded[:end])\n",
        "\n",
        "        return translated.replace('P', '')\n",
        "\n",
        "    print('test')\n",
        "    print('man -> ', translate('man'))\n",
        "    print('mans-> ', translate('mans'))\n",
        "    print('king ->', translate('king'))\n",
        "    print('black ->', translate('black'))\n",
        "    print('upp ->', translate('upp'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOFPTfuoNv3U",
        "outputId": "dc3cf27b-ba02-41a7-8d5a-bcb8b8757623"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1000 cost:0.003316\n",
            "Epoch: 2000 cost:0.000902\n",
            "Epoch: 3000 cost:0.000384\n",
            "Epoch: 4000 cost:0.000192\n",
            "Epoch: 5000 cost:0.000104\n",
            "test\n",
            "man ->  women\n",
            "mans->  women\n",
            "king -> queen\n",
            "black -> white\n",
            "upp -> down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 참조"
      ],
      "metadata": {
        "id": "MbCTC5FciBp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `output, hidden = nn.RNN()`\n",
        "    - https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "    - output: RNN의 각 시점(time step)마다 계산된 **최종 계층(last layer)의 은닉 상태(hidden state)**들을 모두 모아놓은 텐서\n",
        "    - hidden: RNN이 모든 시퀀스를 처리한 후, **마지막 시점(last time step)에서의 모든 계층(all layers)의 은닉 상태**를 담고 있는 텐서"
      ],
      "metadata": {
        "id": "pDydOE-SU0fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `unsqueeze()`: 1인 차원을 생성하는 함수"
      ],
      "metadata": {
        "id": "_pczv5Hdbbr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Test"
      ],
      "metadata": {
        "id": "q9TSnj5ihoaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohr0Nn-YO4qS",
        "outputId": "fa2baa19-5967-4e1e-f350-13b75c52cc78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = [1,2,4]\n",
        "torch.FloatTensor(target).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oubY5mJOO5G1",
        "outputId": "85f4011a-bbb5-4a5a-9c1f-9ef05ee3283f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
        "num_dic = {n:i for i, n in enumerate(char_arr)} # char2index\n",
        "num_dic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQFL3d_Y7Lr",
        "outputId": "ad95c5e5-f689-4dc0-8d5d-c7036ab6fedc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'S': 0,\n",
              " 'E': 1,\n",
              " 'P': 2,\n",
              " 'a': 3,\n",
              " 'b': 4,\n",
              " 'c': 5,\n",
              " 'd': 6,\n",
              " 'e': 7,\n",
              " 'f': 8,\n",
              " 'g': 9,\n",
              " 'h': 10,\n",
              " 'i': 11,\n",
              " 'j': 12,\n",
              " 'k': 13,\n",
              " 'l': 14,\n",
              " 'm': 15,\n",
              " 'n': 16,\n",
              " 'o': 17,\n",
              " 'p': 18,\n",
              " 'q': 19,\n",
              " 'r': 20,\n",
              " 's': 21,\n",
              " 't': 22,\n",
              " 'u': 23,\n",
              " 'v': 24,\n",
              " 'w': 25,\n",
              " 'x': 26,\n",
              " 'y': 27,\n",
              " 'z': 28}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_word = 'test'\n",
        "n_step = 5\n",
        "n_class = len(num_dic)\n",
        "\n",
        "input_batch, output_batch = [], []\n",
        "\n",
        "input_w = input_word + 'P'*(n_step - len(input_word))\n",
        "input = [num_dic[n] for n in input_w]\n",
        "output = [num_dic[n] for n in 'S' + 'P'*n_step]\n",
        "\n",
        "input_batch = np.eye(n_class)[input]    # one-hot\n",
        "output_batch = np.eye(n_class)[output]  # one-hot\n",
        "# input_batch.shape\n",
        "output_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ_vnPkpaTKm",
        "outputId": "a374b89b-2901-4c05-c007-b1f2576f07b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RiWlbqqTanvw"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}