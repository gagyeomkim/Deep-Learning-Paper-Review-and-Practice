{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0ku9l0qUKKoDNJbpx1cfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/code_practice/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Is All You Need"
      ],
      "metadata": {
        "id": "-vLAnIDaGCUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref: https://cpm0722.github.io/pytorch-implementation/transformer"
      ],
      "metadata": {
        "id": "EW0f5qunF4FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 해당 코드는 **Transformer**의 Encoder-Decoder 구조를 이해하기 위한 실습입니다.\n",
        "    - Dropout 기법을 사용하는 부분은 포함되어있지 않습니다."
      ],
      "metadata": {
        "id": "IKweldqjAojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **block**"
      ],
      "metadata": {
        "id": "NadQMFDzGXQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### encoder_block.py"
      ],
      "metadata": {
        "id": "D1i1InKpNU_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn\n",
        "\n",
        "# from models.layer.residual_connection_layer import ResidualConnectionLayer\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention, position_ff, norm):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        self_attention: Multi-Head Attention Layer\n",
        "        position_ff: Position-wise Feed-Forward Layer\n",
        "        \"\"\"\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.position_ff = position_ff\n",
        "        self.residuals = [ResidualConnectionLayer(copy.deepcopy(norm)) for _ in range(2)]  # Residual Connection 2개 생성\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        src: Encoder의 input sentence\n",
        "        src_mask: pad mask\n",
        "        \"\"\"\n",
        "        out = src\n",
        "        # ResidualConnectionLayer에서는 sub_layer의 forward()에 인자를 1개만 받기 떄문에, lambda 식으로 전달\n",
        "        out = self.residuals[0](out, lambda out: self.self_attention(query=out, key=out, value=out, mask=src_mask))\n",
        "        out = self.residuals[1](out, self.position_ff)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ek8ztwJ6Nbdc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `lambda out: self.self_attention(query=out, key=out, value=out, mask=src_mask`은 아래와 동일한 코드이다\n",
        "```python\n",
        "def temp(out):\n",
        "    return self.self_attention(query=out, key=out, value=out, mask=src_mask\n",
        "```"
      ],
      "metadata": {
        "id": "bmivQg9yrEjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoder_block.py"
      ],
      "metadata": {
        "id": "YnHm4Zs_wSMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "# from models.layer.residual_connection_layer import ResidualConnectionLayer\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention, cross_attention, position_ff, norm):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_attention\n",
        "        self.position_ff = position_ff\n",
        "        self.residuals = [ResidualConnectionLayer(copy.deepcopy(norm)) for _ in range(3)]\n",
        "\n",
        "    def forward(self, tgt, encoder_out, tgt_mask, src_tgt_mask):\n",
        "        out = tgt\n",
        "        out = self.residuals[0](out, lambda out: self.self_attention(query=out, key=out, value=out, mask=tgt_mask))\n",
        "        out = self.residuals[1](out, lambda out: self.cross_attention(query=out, key=encoder_out, value=encoder_out, mask=src_tgt_mask))\n",
        "        out = self.residuals[2](out, self.position_ff)\n",
        "        return out"
      ],
      "metadata": {
        "id": "msv8NkKIwVfU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **embedding**"
      ],
      "metadata": {
        "id": "vvGaQ0Q6GN8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transformer_embedding.py"
      ],
      "metadata": {
        "id": "dzaV7oORxryJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `nn.Sequential()`: input으로 준 module에 대해 순차적으로 forward() method를 호출해주는 역할"
      ],
      "metadata": {
        "id": "HM7McBjtx_gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, token_embed, pos_embed):\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.embedding = nn.Sequential(token_embed, pos_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        x: (n_batch, seq_len)의 shape을 가진 embedding 처리 전 sentence\n",
        "        \"\"\"\n",
        "        out = self.embedding(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rOV_cDf_xvsF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### token_embedding.py"
      ],
      "metadata": {
        "id": "f7dxYuS0yTIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, d_embed, vocab_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
        "        self.d_embed = d_embed\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x) * math.sqrt(self.d_embed)   # 위치정보에 토큰 정보가 묻히는 것을 방지하기 위해 scailing\n",
        "        return out"
      ],
      "metadata": {
        "id": "5Cnsv4DoySF8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### positional_encoding.py"
      ],
      "metadata": {
        "id": "PQ6j3qU9y1_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embed, max_len=256, device=torch.device(\"cpu\")):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # encoding: (max_len, d_embed)\n",
        "        encoding = torch.zeros(max_len, d_embed)\n",
        "        encoding.requires_grad=False\n",
        "        # position: (max_len, 1)\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
        "        # 짝수 index에는 sin 함수 사용\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        # 홀수 index에는 cos 함수 사용\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        # encoding: (1, max_len, d_embed)\n",
        "        self.encoding = encoding.unsqueeze(0).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (n_batch, seq_len, d_embed)\n",
        "        _, seq_len, _ = x.size()\n",
        "        # positional 정보를 일정한 범위 안의 실수로 제약\n",
        "        pos_embed = self.encoding[:, :seq_len, :]\n",
        "        out = x + pos_embed\n",
        "        return out"
      ],
      "metadata": {
        "id": "TvXwx4AczSHo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `torch.arange(0, d_emebd, 2)`: 2i\n",
        "- `-(math.log(10000.0) / d_embed)`: $- \\frac{\\ln10000}{d_{embed}}$\n",
        "    - `math.log()`의 base의 default는 $e$\n",
        "- `torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed)`: $2i * - \\frac{ln10000}{d_{embed}}$\n",
        "- `torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed)`: $exp(2i * - \\frac{ln10000}{d_{embed}})$\n",
        "- 식 전개시 아래와 같이 변형된다\n",
        "\n",
        "$$exp(-\\frac{2i}{d_{embed}} * \\ln 10000)$$\n",
        "\n",
        "$$exp(\\ln 10000^{-\\frac{2i}{d_{embed}}})$$\n",
        "\n",
        "$$10000^{-\\frac{2i}{d_{embed}}}$$\n",
        "\n",
        "$$\\frac{1}{10000^{\\frac{2i}{d_{embed}}}}$$"
      ],
      "metadata": {
        "id": "XluDp8V_2Fto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **model**"
      ],
      "metadata": {
        "id": "rlumTMnOGMSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### encoder.py"
      ],
      "metadata": {
        "id": "HKqLyq97KVcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_block, n_layer, norm):\n",
        "        \"\"\"\n",
        "        Encoder 인스턴스를 초기화합니다.\n",
        "\n",
        "        파라미터:\n",
        "        encoder_block: Encoder Block 인스턴스 1개\n",
        "        n_layer: Encoder Block의 개수\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_block) for _ in range(self.n_layer)])\n",
        "        self.norm = norm\n",
        "\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Encoder Block들을 순서대로 실행하면서,\n",
        "        이전 block의 output을 이후 block의 input으로 넣습니다.\n",
        "\n",
        "        파라미터:\n",
        "        src: Encoder의 input sentence\n",
        "        src_mask: pad mask\n",
        "\n",
        "        리턴값:\n",
        "        out: 마지막 block의 output, 즉, context\n",
        "        \"\"\"\n",
        "        out = src\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, src_mask)\n",
        "        out = self.norm(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ws9BMqdSKiTL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decoder block마다 LayerNormalization을 적용해주었는데, Decoder이 끝나고도 적용해준 이유  \n",
        ": 모델의 안정성과 성능을 위한 의도적인 설계로 추가해주었다고 함"
      ],
      "metadata": {
        "id": "iqe_uVLaATt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### decoder.py"
      ],
      "metadata": {
        "id": "1Ieu_0VatGM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_block, n_layer, norm):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layer = n_layer\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(decoder_block) for _ in range(self.n_layer)])    # Module들을 Python list에 넣어 보관한다면, 꼭 마지막에 이들을 nn.ModuleList로 wrapping 해줘야 한다.\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt, encoder_out, tgt_mask, src_tgt_mask):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        encoder_out: Encoder에서 생성된 최종 output. 즉, context\n",
        "        tgt_mask: Decoder의 input으로 주어지는 target sentence의 subsequent + pad masking.\n",
        "                  self-Multi-Head attention layer에서 사용됨\n",
        "        src_tgt_mask: Self-Multi-Head Attention Layer에서 넘어온 query, Encoder에서 넘어온 key, value 사이의 pad masking\n",
        "        \"\"\"\n",
        "        out = tgt\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, encoder_out, tgt_mask, src_tgt_mask)\n",
        "        out = self.norm(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0Fa7FNfvtGE-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### transformer.py"
      ],
      "metadata": {
        "id": "B-OgiTkHKYX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "transformer.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer의 전체 구조에 관련된 클래스입니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_embed, tgt_embed, encoder, decoder, generator):\n",
        "        \"\"\"\n",
        "        Transformer 인스턴스를 초기화합니다\n",
        "\n",
        "        파라미터:\n",
        "        encoder: 인코더 인스턴스\n",
        "        decoder: 디코더 인스턴스\n",
        "        generator: Decoder의 output dimension 변경을 위한 FC Layer\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.generator = generator\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Transformer의 Encode 부분입니다.\n",
        "\n",
        "        파라미터:\n",
        "        src: sentence\n",
        "        src_mask: pad mask\n",
        "        \"\"\"\n",
        "        out = self.encoder(self.src_embed(src), src_mask)\n",
        "        return out\n",
        "\n",
        "    def decode(self, tgt, encoder_out, tgt_mask, src_tgt_mask):\n",
        "        \"\"\"\n",
        "        Transformer의 decode 부분입니다.\n",
        "\n",
        "        파라미터:\n",
        "        tgt: sentence\n",
        "        encoder_out: encoder의 최종 output인 context vector\n",
        "        tgt_mask: Decoder의 input sentence에 대한 mask\n",
        "        \"\"\"\n",
        "        out = self.decode(self.tgt_embed(tgt), encoder_out, tgt_mask, src_tgt_mask)\n",
        "        return out\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Encoder의 input sentence인 src에 Encoding을 적용하여 context를 만들고\n",
        "        Decoder의 input sentence인 tgt와 context에 Decoding을 진행합니다.\n",
        "\n",
        "        파라미터:\n",
        "        src: 인코더의 input sentence\n",
        "        tgt: 디코더의 input sentence\n",
        "        \"\"\"\n",
        "        # src_mask: src에 대한 pad mask\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask =  self.make_tgt_mask(tgt)\n",
        "        src_tgt_mask = self.make_src_tgt_mask(src, tgt)\n",
        "        encoder_out = self.encode(src, src_mask)\n",
        "        # decode_out: (n_batch, seq_len, d_embed)\n",
        "        decode_out = self.decode(tgt, encoder_out, tgt_mask, src_tgt_mask)\n",
        "        # out: (n_batch, seq_len)\n",
        "        out = self.generator(decode_out)\n",
        "        out = F.log_softmax(out, dim=-1)    # 마지막 dimension인 len(vocab)에 대해 확률값 계산\n",
        "        return out, decoder_out\n",
        "\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        pad_mask = self.make_pad_mask(src, src)\n",
        "        return pad_mask\n",
        "\n",
        "    def make_subsequent_mask(query, key):\n",
        "        \"\"\"\n",
        "        (teacher forcing을 위한) subsequent masking\n",
        "        파라미터:\n",
        "        query: (n_batch, query_seq_len)\n",
        "        key: (n_batch, key_seq_len)\n",
        "        \"\"\"\n",
        "        query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "        tril = np.tril(np.ones((query_seq_len, key_seq_len)), k=0).astype('uint8')  # lower triangle without diagonal\n",
        "        mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device=query.device)\n",
        "        return mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        \"\"\"\n",
        "        Decoder의 subsequent + pad masking\n",
        "        \"\"\"\n",
        "        pad_mask = self.make_pad_mask(tgt, tgt)\n",
        "        seq_mask = self.make_subsequent_mask(tgt, tgt)\n",
        "        mask = pad_mask & seq_mask\n",
        "        return pad_mask & seq_mask\n",
        "\n",
        "    def make_src_tgt_mask(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Self-Multi-Head Attention Layer에서 넘어온 query, Encoder에서 넘어온 key, value 사이의 pad masking\n",
        "        \"\"\"\n",
        "        pad_mask = self.make_pad_mask(tgt, src)\n",
        "        return pad_mask\n",
        "\n",
        "    def make_pad_mask(self, query, key, pad_idx=1):\n",
        "        \"\"\"\n",
        "        pad masking을 만드는 함수\n",
        "\n",
        "        파라미터:\n",
        "        query: (n_batch, query_seq_len)\n",
        "        key: (n_batch, key_seq_len)\n",
        "        pad_idx=1: <PAD> 토큰을 '1'로 나타낼 것\n",
        "        \"\"\"\n",
        "        query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "\n",
        "        # key_mask의 key_seq_len 차원에 패딩 여부가 표시됨\n",
        "        key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)    # (n_batch, 1, 1, key_seq_len)\n",
        "        key_mask = key_mask.repeat(1, 1, query_seq_len, 1)  # (n_batch, 1, query_seq_len, key_seq_len)\n",
        "\n",
        "        # query_mask의 query_seq_len 차원에 패딩 여부가 표시됨\n",
        "        query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(2)    # (n_batch, 1, query_seq_len, 1)\n",
        "        query_mask = query_mask.repeat(1, 1, 1, key_seq_len)  # (n_batch, 1, query_seq_len, key_seq_len)\n",
        "\n",
        "        # Query와 Key가 둘다 실제 단어여야 Attention 계산이 의미를 가짐\n",
        "        mask = key_mask & query_mask\n",
        "        mask.requires_grad = False\n",
        "        return mask\n",
        "\n"
      ],
      "metadata": {
        "id": "KL3-gjkYHvd0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# query_seq_len과 key_seq_len이 모두 10일 때 np.tril의 결과\n",
        "np.tril(np.ones((10, 10)), k=0).astype('uint8') # k=0이면 주대각선 위 모든 원소들을 0로 설정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "t_MNJLS-pL5y",
        "outputId": "22582717-cc2e-40ed-a694-3db218160009"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-2fcb2492-3a94-48c3-acf2-8ed3108ca495\" class=\"ndarray_repr\"><pre>ndarray (10, 10) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAAAAACoWZBhAAAAJUlEQVR4nEXGsREAMBDCMJv9h/4iF1Al5AuufehDH/pXXHHFFQ8GcgAdbuHWkwAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-2fcb2492-3a94-48c3-acf2-8ed3108ca495 button').onclick = (e) => {\n",
              "        document.querySelector('#id-2fcb2492-3a94-48c3-acf2-8ed3108ca495').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-2fcb2492-3a94-48c3-acf2-8ed3108ca495 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `.ne`: not equal. 같지 않으면 True. 즉 pad_idx가 아닌 곳들은 `True`로, pad_idx인 곳들은 `False`로"
      ],
      "metadata": {
        "id": "Rtz6FyhxoB0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **layer**"
      ],
      "metadata": {
        "id": "tRQRm1V1GV3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention Code in Pytorch"
      ],
      "metadata": {
        "id": "JqgzJc2fdUhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_attention(query, key, value, mask):\n",
        "    \"\"\"\n",
        "    Self-Attention을 수행\n",
        "\n",
        "    파라미터:\n",
        "    query, key, value: (n_batch, seq_len, d_k)\n",
        "    mask: (n_batch, seq_len, seq_len)   # pad mask\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "    # attention_score: (n_batch, seq_len, seq_len)\n",
        "    attention_score = torch.matmul(query, key.transpose(-2, -1))    # Q x K^T\n",
        "    attention_score = attention_score / math.sqrt(d_k)\n",
        "    if mask is None:\n",
        "        attention_score = attention_score.masked_fill(mask==0, -1e9)\n",
        "    attention_prob = F.softmax(attention_score, dim=-1) # (n_batch, seq_len, seq_len)\n",
        "    out = torch.matmul(attention_prob, value)   # (n_batcch, seq_len, d_k)\n",
        "    return out"
      ],
      "metadata": {
        "id": "WVCdOXrbdUIb"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### multi_head_attention_layer.py"
      ],
      "metadata": {
        "id": "Vl6OgD3TdKj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, qkv_fc, out_fc):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        d_model: d_k * h\n",
        "        h: attenntion 수행 횟수. 논문에선 h=8\n",
        "        qkv_fc: (d_embed, d_model)의 weight matrix를 갖는 FC Layer 인스턴스\n",
        "                # deepcopy로 Q, K, V에 대해 서로 다른 weight를 갖고 운용할 것\n",
        "\n",
        "        out_fc: (d_model, d_embed)의 weight matrix를 갖는 attention 계산 이후 거쳐가는 FC Layer 인스턴스\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.q_fc = copy.deepcopy(qkv_fc)   # (d_embed, d_model)\n",
        "        self.k_fc = copy.deepcopy(qkv_fc)   # (d_embed, d_model)\n",
        "        self.v_fc = copy.deepcopy(qkv_fc)   # (d_embed, d_model)\n",
        "        self.out_fc = out_fc    # (d_model, d_k)\n",
        "\n",
        "    def calculate_attention(query, key, value, mask):\n",
        "        \"\"\"\n",
        "        분리된 각 head에 대해서, Self-Attention을 수행\n",
        "\n",
        "        파라미터:\n",
        "        query, key, value: (n_batch, h, seq_len, d_k)\n",
        "        mask: (n_batch, 1, seq_len, seq_len)   # pad mask\n",
        "        \"\"\"\n",
        "        d_k = key.shape[-1]\n",
        "        # attention_score: (n_batch,h, seq_len, seq_len)\n",
        "        attention_score = torch.matmul(query, key.transpose(-2, -1))    # Q x K^T\n",
        "        attention_score = attention_score / math.sqrt(d_k)\n",
        "        if mask is None:\n",
        "            attention_score = attention_score.masked_fill(mask==0, -1e9)\n",
        "        attention_prob = F.softmax(attention_score, dim=-1) # (n_batch, h, seq_len, seq_len)\n",
        "        out = torch.matmul(attention_prob, value)   # (n_batcch, h, seq_len, d_k)\n",
        "        return out\n",
        "\n",
        "    def forward(self, *args, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        query, key, value: (n_batch, seq_len, d_embed)\n",
        "        mask: (n_batch, seq_len, seq_len)\n",
        "\n",
        "        리턴값:\n",
        "        out: (n_batch, h, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        n_batch = query.size(0)\n",
        "\n",
        "        def transform(x, fc):\n",
        "            \"\"\"\n",
        "            파라미터\n",
        "            x: (n_batch, seq_len, d_embed)\n",
        "            fc: (d_embed, d_model)의 weight matrix를 갖는 FC Layer\n",
        "            \"\"\"\n",
        "            # out: (n_batch, seq_len, d_model)\n",
        "            out = fc(x)\n",
        "            # out: (n_batch, seq_len, h, d_k)\n",
        "            out = out.view(n_batch, -1, self.h, self.d_model//self.h)\n",
        "            # out: (n_batch, h, seq_len, d_k)\n",
        "            out = out.transpose(1, 2)\n",
        "            return out\n",
        "\n",
        "        # query, key, value: (n_batch, h, seq_len, d_k)\n",
        "        query = transform(query, self.q_fc)\n",
        "        key = transform(key, self.k_fc)\n",
        "        value = transform(value, self.v_fc)\n",
        "\n",
        "        # out: (n_batch, h, seq_len, d_k)\n",
        "        out = self.calculate_attention(query, key, value, mask)\n",
        "        # out: (n_batch, seq_len, h, d_k)\n",
        "        out = out.transpose(1, 2)\n",
        "        # out: (n_batch, seq_len, d_model)\n",
        "        out = out.contiguous().view(n_batch, -1, self.d_model)\n",
        "        # out: (n_batch, seq_len, d_embed)\n",
        "        out = self.out_fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TjkSaqgKYP65"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### position_wise_feed_forward_layer.py"
      ],
      "metadata": {
        "id": "W0Q9fm-lo_oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    def __init__(self, fc1, fc2):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        fc1: (d_embed, d_ff)의 weight matrix를 갖는 FC Layer\n",
        "        fc2: (d_ff, d_embed)의 weight matrix를 갖는 FC Layer\n",
        "        \"\"\"\n",
        "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
        "        self.fc1 = fc1\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = fc2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "urQfqP3ApOxM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### residual_connection_layer.py"
      ],
      "metadata": {
        "id": "_9t8qagVpyUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResidualConnectionLayer(nn.Module):\n",
        "    def __init__(self, norm):\n",
        "        super(ResidualConnectionLayer, self).__init__()\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, x, sub_layer):\n",
        "        \"\"\"\n",
        "        파라미터:\n",
        "        x: (n_batch, seq_len, d_embed)크기의 input\n",
        "        sub_layer: Encoder Block의 sub layer\n",
        "        \"\"\"\n",
        "        out = x\n",
        "        out = sub_layer(out)\n",
        "        out = out + x\n",
        "        out = self.norm(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f-RsnkyVp1NM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **build model**"
      ],
      "metadata": {
        "id": "B76LfIfqGTi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from models.model.transformer import Transformer\n",
        "# from models.model.encoder import Encoder\n",
        "# from models.model.decoder import Decoder\n",
        "# from models.block.encoder_block import EncoderBlock\n",
        "# from models.block.decoder_block import DecoderBlock\n",
        "# from models.layer.multi_head_attention_layer import MultiHeadAttentionLayer\n",
        "# from models.layer.position_wise_feed_forward_layer import PositionWiseFeedForwardLayer\n",
        "# from models.embedding.transformer_embedding import TransformerEmbedding\n",
        "# from models.embedding.token_embedding import TokenEmbedding\n",
        "# from models.embedding.positional_encoding import PositionalEncoding\n",
        "\n",
        "def build_model(src_vocab_size,\n",
        "                tgt_vocab_size,\n",
        "                device=torch.device(\"cpu\"),\n",
        "                max_len=256,\n",
        "                d_embed=512,\n",
        "                n_layer=6,\n",
        "                d_model=512,\n",
        "                h=8,\n",
        "                d_ff=2048,\n",
        "                dr_rate=0.1,\n",
        "                norm_eps=1e-5):\n",
        "    import copy\n",
        "    copy = copy.deepcopy\n",
        "    src_token_embed = TokenEmbedding(d_embed=d_embed,\n",
        "                                     vocab_size=src_vocab_size)\n",
        "\n",
        "    tgt_token_embed = TokenEmbedding(d_embed=d_embed,\n",
        "                                     vocab_size=tgt_vocab_size)\n",
        "\n",
        "    pos_embed = PositionalEncoding(d_embed=d_embed,\n",
        "                                   max_len=max_len,\n",
        "                                   device=device)\n",
        "\n",
        "    src_embed = TransformerEmbedding(token_embed = src_token_embed,\n",
        "                                     pos_embed = copy(pos_embed))\n",
        "\n",
        "    tgt_embed = TransformerEmbedding(token_embed=tgt_token_embed,\n",
        "                                     pos_embed=copy(pos_embed))\n",
        "\n",
        "    attention = MultiHeadAttentionLayer(d_model=d_model,\n",
        "                                        h=h,\n",
        "                                        qkv_fc=nn.Linear(d_embed, d_model),\n",
        "                                        out_fc = nn.Linear(d_model, d_embed))\n",
        "\n",
        "    position_ff = PositionWiseFeedForwardLayer(fc1 = nn.Linear(d_embed, d_ff),\n",
        "                                               fc2 = nn.Linear(d_ff, d_embed))\n",
        "\n",
        "    # Layer Normalization: Encoder block과 Decoder block의 각 layer마다 수행함\n",
        "    norm = nn.LayerNorm(d_embed, eps=norm_eps)\n",
        "\n",
        "    encoder_block = EncoderBlock(self_attention=copy(attention),\n",
        "                                 position_ff = copy(position_ff),\n",
        "                                 norm=copy(norm))\n",
        "\n",
        "    decoder_block = DecoderBlock(self_attention = copy(attention),\n",
        "                                 cross_attention = copy(attention),\n",
        "                                 position_ff = copy(position_ff),\n",
        "                                 norm=copy(norm))\n",
        "\n",
        "    encoder = Encoder(encoder_block=encoder_block,\n",
        "                      n_layer=n_layer,\n",
        "                      norm=copy(norm))\n",
        "\n",
        "    decoder = Decoder(decoder_block=decoder_block,\n",
        "                      n_layer=n_layer,\n",
        "                      norm=copy(norm))\n",
        "\n",
        "    generator = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    model = Transformer(src_embed=src_embed,\n",
        "                        tgt_embed=tgt_embed,\n",
        "                        encoder=encoder,\n",
        "                        decoder=decoder,\n",
        "                        generator=generator).to(device)\n",
        "\n",
        "    model.device=device\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rzszEY2X5qCv"
      },
      "execution_count": 61,
      "outputs": []
    }
  ]
}