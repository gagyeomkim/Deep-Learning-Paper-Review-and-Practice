## 딥러닝 논문 리뷰 &amp; 코드 실습: Deep-Learning-Paper-Review-and-Practice
- 딥러닝 논문 리뷰 및 코드 실습을 진행한 저장소입니다.

**Natural Language Processing (자연어 처리)**

- [Seq2Seq] Sequence to Sequence Learning with Neural Networks (2014)
  - [Paper Link](https://arxiv.org/abs/1409.3215) | [Paper Review](https://deep-learning-paper-review-and-practice.notion.site/Seq2Seq-Sequence-to-Sequence-Learning-with-Neural-Networks-229ab43529d9807ea187f49b4f733012?source=copy_link) | [Summary](https://github.com/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/summary_pdf/2025-07-03-Sequence%E2%80%91to%E2%80%91Sequence.pdf) | [Code Practice](https://github.com/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/code_practice/Sequence_to_Sequence_with_LSTM.ipynb)

- [Attention] Neural Machine Translation by Jointly Learning to Align and Translate(2014)
  - [Paper Link](https://arxiv.org/abs/1409.0473) | [Paper Review](https://deep-learning-paper-review-and-practice.notion.site/Attention-Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-229ab43529d980ae9800ff2f150c21b3?source=copy_link) | [Summary](https://github.com/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/summary_pdf/2025-07-04-Attention%20Mechanism.pdf) | [Code Practice]()
 
- [Transformer] Attention Is All You Need(2017)
  - [Paper Link](https://arxiv.org/abs/1706.03762) | [Paper Review](https://deep-learning-paper-review-and-practice.notion.site/Transformer-Attention-Is-All-You-Need-228ab43529d9801ca912da8d7aa52e77?source=copy_link) | [Summary](https://github.com/gagyeomkim/Deep-Learning-Paper-Review-and-Practice/blob/main/summary_pdf/2025-07-05-Transformer.pdf)  |[Code Practice]()

**Ref**
- 모든 Summary 자료는 직접 작성하였으며, 사용된 설명 및 사진은 [딥러닝 파이토치 교과서](https://wikidocs.net/book/2788)의 공개 내용을 참고하였습니다.
